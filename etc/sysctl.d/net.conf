# /proc/sys/net/core/*

# 启用 BPF 实时 （JIT） 编译器
# BPF 是一种灵活的和高效的基础设施，允许在各种钩点
# 它被用于许多 Linux 内核子系统，例如作为网络（例如 XDP、tc）、跟踪（例如 kprobes、uprobes、tracepoints）和安全性（例如 seccomp）
# LLVM 有一个可以编译的 BPF 后端将 C 限制为一系列 BPF 指令
# 程序加载后通过 bpf（2） 并在内核中传递一个验证器，然后 JIT 将将这些 BPF 程序转换为本机 CPU 指令。
# 0 - 禁用 JIT（默认值）
# 1 - 启用 JIT
# 2 - 启用 JIT 并要求编译器在内核日志上发出跟踪
# net.core.bpf_jit_enable = 1

# 强化 BPF JIT 编译器
# 启用强化会牺牲性能，但可以减少 JIT 喷洒
# 0 - 禁用 JIT 强化（默认值）
# 1 - 仅为非特权用户启用 JIT 强化
# 2 - 为所有用户启用 JIT 强化
net.core.bpf_jit_harden  =  2

# 用于网络设备的默认排队规则
# 请注意，物理多队列接口仍然使用 MQ 作为根 qdisc，而 qq 又将此默认值用于其叶
# 虚拟设备（例如 lo 或 veth）会忽略此设置，而是默认为 noqueue。
# pfifo_fast 是众多发行版的默认参数，它实现简单，很多网卡可以 offload 而减少 CPU 开销
# 但它默认队列过长很容易引起 bufferbloat，不能识别网络流可能导致部分流被饿死
# fq (fair queue) 针对了 pfifo 及其衍生算法的缺点，它将每个 socket 的数据称为一个流，以流为单位进行隔离，分别进行 pacing 以期望得到公平发送的效果
# codel 是一种针对 bufferbloat 设计的算法，使用 BQL 动态控制 buffer 大小，自动区分“好流”和“坏流”
# fq_codel 对非常小的流进行了优化避免饿死问题
# cake 是 codel / fq_codel 的后继者，内建 HTB 算法进行流量整形而克服了原版 HTB 难以配置的问题，号称能做 fq_codel 能做的一切且比 fq_codel 做得更好，在 4.19 中被引入内核
# 终端设备（内容的生产者或最终接收者，而不是转发设备）的 qdisc 更适合 fq
# 转发设备更适合 codel 及其衍生算法
# 根据一些不完全的实践，bbr 配合 fq_codel 工作的很好
# cake 理论上可作为 fq_codel 的替代，但是目前没有太多实践资料可供参考。
# net.core.default_qdisc = fq
# net.core.default_qdisc = fq_codel
net.core.default_qdisc = cake

# 接收队列的大小
# net.core.netdev_max_backlog = 1024

# 最大连接数
# net.core.somaxconn = 4096

# /proc/sys/net/ipv4/*

# 在接口之间转发数据包
net.ipv4.ip_forward = 1
net.ipv4.conf.all.forwarding = 1
net.ipv4.conf.default.forwarding = 1

# 禁用 ICMP 重定向
net.ipv4.conf.all.accept_redirects = 0
net.ipv4.conf.default.accept_redirects = 0
net.ipv4.conf.all.send_redirects = 0
net.ipv4.conf.default.send_redirects = 0

# 反向路径过滤
# 值 1 严格模式 2 宽松模式
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1

# 设置拥塞控制算法
# 对于被动连接，侦听器拥塞控制选择是遗传的
# 设置值时请参考下面两个变量
# tcp_allowed_congestion_control
# tcp_available_congestion_control
# 基于丢包反馈（Reno）
# 分为 cwnd 指数增长的慢启动阶段、cwnd 超过 ssthresh 后线性增加的拥塞避免阶段、收到 dup ACK 后 cwnd 折半进入快速恢复阶段，如果快速恢复超时则进入慢启动，以此循环
# 基于丢包反馈的改进型（STCP、BIC、Cubic）
# 基本都通过设定一些参数改进乘性减阶段和快速恢复速度，避免流量和 Reno 一样出现锯齿形波动，以提升高带宽下的链路利用率
# 目前 Linux 内核默认使用的 Cubic 就是使用了三次函数代替简单二分而得名
# 基于延迟变化（Vegas 和 Westwood ）
# Vegas 通过 RTT 变化判断是否出现拥塞，RTT 增加则 cwnd 减少，RTT 减少则 cwnd 增加，这种算法在与其他算法共用链路时会显得过于“绅士”而吃亏
# Westwood 则通过 ACK 达到率判断链路利用率上限，适用于无线网络但无法区分网络拥塞还是无线抖动而普适性较低
# 基于主动探测（BBR）
# BBR 旨在通过主动探测消除 bufferbloat 对上述各大算法的误判影响
# 不要无脑推崇 bbr，bbr 并不适合任何网络环境
# 设计上 bbr 配合了 http/2 的大流传输特点，因此拥有启动快抖动小等优点，且由于 bbr 并不以丢包为减小窗口的判断依据，bbr 能很好的对付大带宽传输中的少量丢包
# 但是在总可用带宽较小时，bbr 的降速不够迅速准确，自身带宽利用率低还会影响同一网络下的其他流
# 因此 bbr 适合丢包率不太高（bbr 算法硬编码了一个丢包率20%，超过门限会导致 bbr 自行降速）的长胖管道（Long-Fat Network，指延迟较高且带宽较大）
# net.ipv4.tcp_congestion_control = cubic
# net.ipv4.tcp_congestion_control = westwood
# net.ipv4.tcp_congestion_control = bbr

# 控制 TCP 对显式拥塞通知 （ECN） 的使用
# 仅当 TCP 连接的两端都指示 ECN 时才使用支持它
# 此功能有助于避免因通过允许支持路由器发出信号来缓解拥塞在必须丢弃数据包之前出现拥塞
# 默认 2
# 0 禁用 ECN
#   既不启动也不接受ECN
# 1 当传入连接请求时启用 ECN
#   还会在传出连接尝试时请求 ECN
# 2 在传入连接请求时启用 ECN
#   不在传出连接上请求 ECN
net.ipv4.tcp_ecn = 1

# 如果内核检测到 ECN 连接行为异常，则回退到非ECN
# 目前，此旋钮实现回退从RFC3168
# 如果tcp_ecn或路由（或拥塞）ECN 设置被禁用，则不使用该值
# 默认 1 启用
net.ipv4.tcp_ecn_fallback = 1

# 启用 TCP Fast Open
net.ipv4.tcp_fastopen = 3
# 当发生 TFO 防火墙黑洞问题时,在活动 TCP 套接字上禁用 TCP Fast Open 的初始时间段（以秒为单位）
net.ipv4.tcp_fastopen_blackhole_timeout_sec = 60

# 指定在强制关闭 socket 之前需要等待的最终 FIN 数据包时间（秒数）
net.ipv4.tcp_fin_timeout = 60

# 启用RFC5682中定义的前向 RTO 恢复 （F-RTO）。 F-RTO 是一种用于 TCP 重传的增强恢复算法 超时。 这在网络中有 RTT 波动（例如，无线）时有效。F-RTO仅是发送方 修改。它不需要对等方的任何支持。
# 默认情况下，它使用非零值启用。
# 1 开启基本版本的F-RTO算法
# 2 如果流使用SACK的话，开启SACK-增强的F-TRO算法
net.ipv4.tcp_frto = 2

# 启用 keepalive 时 TCP 发送 keepalive 消息的频率。 默认值：2 小时
net.ipv4.tcp_keepalive_time = 60
# 探测器发出的频率。默认值：75秒
net.ipv4.tcp_keepalive_intvl = 10
# TCP 发出多少个 keepalive 探测器，直到它决定断开连接。默认值：9。
net.ipv4.tcp_keepalive_probes = 6

# 挂起连接等待确认的最大队列长度
net.ipv4.tcp_max_syn_backlog = 1024

# 如果设置，TCP 将执行接收缓冲区自动调整
# 并尝试自动调整缓冲区的大小（不大于 tcp_rmem[2]）为匹配路径所需的大小，以实现完全吞吐量
# 默认启用
# sysctl -w net.ipv4.tcp_moderate_rcvbuf=1

# 启用 MTU 探测
# 1  =  默认禁用, 在检测到 ICMP black hole 时启用
# 2  =  始终启用
net.ipv4.tcp_mtu_probing = 2

# plb
# 默认值：FALSE
# 如果设置了，并且底层拥塞控制（例如 DCTCP）支持PLB，TCP PLB（保护负载均衡）启用
# 基于PLB参数，在感应到持续拥塞时，TCP 会触发传出 IPv6 数据包的流标签字段
# 流标签的更改字段可能会更改交换机的传出数据包的路径使用 ECMP/WCMP 进行路由
# PLB 更改套接字 txhash，导致 IPv6 流标签更改字段，并且当前没有 IPv4 标头的操作
# 这是可能的将 PLB 应用于 IPv4 与其他网络标头字段（例如 TCP 或 IPv4 选项）或使用外部标头的封装通过开关确定下一跳
# 无论哪种情况，进一步的主机并且需要对交换机端进行更改。
# 设置后，PLB 假定发出拥塞信号（例如 ECN）拥塞控制模块可用并用于估计拥堵措施（例如ce_ratio）
net.ipv4.tcp_plb_enabled = 1

# SACK
# Enable select acknowledgments (SACKS).
net.ipv4.tcp_sack = 1

# 设置 TCP 是否应仅对新连接或空闲时间过长的现有连接开启'以默认窗口大小启动连接'
# 如果已设置，请提供RFC2861行为并超时拥塞空闲时间后的窗口
# 空闲期定义为当前的 RTO
# 如果未设置，拥塞窗口将不会在空闲时间后超时。
# 默认值：1
# 此设置会破坏持久的单连接性能，可以将其关闭
net.ipv4.tcp_slow_start_after_idle = 0

# TCP SYN cookie 保护
net.ipv4.tcp_syncookies = 0

# TCP 时间戳
# 1：启用RFC1323中定义的时间戳，并为每个连接使用随机偏移量而不是只使用当前时间。
# 2：与 1 类似，但没有随机偏移量。
net.ipv4.tcp_timestamps = 1

# 当新时间戳严格大于上一个连接记录的最新时间戳时，TCP 是否应该为新的传出连接重用 TIME-WAIT 状态下的现有连接
# 1 - 全局启用
# 2 - 仅对环回流量启用
# net.ipv4.tcp_tw_reuse = 2

# 启用RFC1323中定义的窗口缩放
net.ipv4.tcp_window_scaling = 1

# /proc/sys/net/ipv6/*

# 套接字选项IPV6_V6ONLY默认值， 将 IPv6 套接字的使用限制为 IPv6 通信
# TRUE：禁用 IPv4 映射地址功能
# FALSE：启用 IPv4 映射地址功能 默认值
# net.ipv6.bindv6only = 0

# 在接口之间转发数据包
net.ipv6.conf.all.forwarding = 1
net.ipv6.conf.default.forwarding = 1

# 禁用 ICMP 重定向
net.ipv6.conf.all.accept_redirects = 0
net.ipv6.conf.default.accept_redirects = 0

# 隐私扩展的首选项 （RFC3041）
# < =  0：禁用隐私扩展
#  =  =  1 ： 启用隐私扩展，但首选公共 临时地址上的地址。
# > 1：启用隐私扩展并首选临时 公共地址上的地址。
# Default: 0（适用于大多数设备） -1（用于点对点设备和环回设备）
net.ipv6.conf.all.use_tempaddr = 2
net.ipv6.conf.default.use_tempaddr = 2
net.ipv6.conf.lo.use_tempaddr = -1

# 定义如何生成链接本地地址和 autoconf 地址。
# 0 基于 EUI64 生成地址（默认）
# 1 不生成链路本地地址，使用 EUI64 作为地址 从 autoconf 生成
# 2 生成稳定的隐私地址，使用来自 stable_secret （RFC7217）
# 3 生成稳定的隐私地址，如果未设置，则使用随机密钥
# net.ipv6.conf.all.addr_gen_mode = 0
# net.ipv6.conf.default.addr_gen_mode = 0
# net.ipv6.conf.all.addr_gen_mode = 2
# net.ipv6.conf.default.addr_gen_mode = 2
net.ipv6.conf.all.addr_gen_mode = 3
net.ipv6.conf.default.addr_gen_mode = 3
net.ipv6.conf.lo.addr_gen_mode = 0
# 用作生成 IPv6 稳定的隐私地址的密钥
# net.ipv6.conf.default.stable_secret = 7add:23e5:9d15:ed5c:6bc8:048f:eec0:3ca0

# /proc/sys/net/mptcp/*

# 启用 Multi-path tcp
net.mptcp.enabled = 1

# 启用MPTCP传输层中数据序列号校验和（DSS-checksum）
# DSS-checksum主要和传输的可靠性相关
# 只要通信对端中有一端开启，就会执行。
net.mptcp.checksum_enabled = 1

# /proc/sys/net/netfilter/*

# 验证传入数据包的校验和
# 校验和错误的数据包处于无效状态
# 如果启用此选项，则不会考虑对此类数据包进行连接跟踪
# 0 - disabled
# not 0 - enabled (default)
net.netfilter.nf_conntrack_checksum = 1

# nf_conntrack_generic_timeout - INTEGER (seconds)
# 一般超时的默认值
# 这是指第4层未知/不受支持的协议
# default 600 sec
net.netfilter.nf_conntrack_generic_timeout = 60

# default 30
net.netfilter.nf_conntrack_icmp_timeout = 10

# default 30
net.netfilter.nf_conntrack_icmpv6_timeout = 10

# 记录哪类无效数据包
# 0 - disable (default)
# 1 - log ICMP packets
# 6 - log TCP packets
# 17 - log UDP packets
# 33 - log DCCP packets
# 41 - log ICMPv6 packets
# 136 - log UDPLITE packets
# 255 - log packets of any protocol
# net.netfilter.nf_conntrack_log_invalid = 0

# nf_conntrack_tcp_timeout_established - INTEGER (seconds)
# default 432000 (5 days)
net.netfilter.nf_conntrack_tcp_timeout_established = 150

# nf_conntrack_tcp_timeout_fin_wait - INTEGER (seconds)
# default 120
net.netfilter.nf_conntrack_tcp_timeout_fin_wait = 60

# nf_conntrack_tcp_timeout_syn_sent - INTEGER (seconds)
# default 120
net.netfilter.nf_conntrack_tcp_timeout_syn_sent = 60

# nf_conntrack_tcp_timeout_time_wait - INTEGER (seconds)
# default 120
net.netfilter.nf_conntrack_tcp_timeout_time_wait = 60

# 启用连接跟踪流时间戳
# 0 - disabled (default)
# not 0 - enabled
net.netfilter.nf_conntrack_timestamp = 1
